{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f50e10b-15f2-4f8c-8087-12c1c3163162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: C:\\cv_partb\n",
      "Here: ['.git', '.gitattributes', '.gitignore', '.ipynb_checkpoints', 'data', 'data_raw', 'diagnostics.ipynb', 'labels.json', 'notebooks', 'outputs', 'README.md', 'requirements.txt', 'streamlit_app.py']\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"Here:\", os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0071fb37-e69b-4161-b7a8-d3e1e215ad90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.json (len=4): ['Tomato_Bacterial_spot', 'Tomato_Early_blight', 'Tomato_Late_blight', 'Tomato_healthy']\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "with open(\"labels.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    labels = json.load(f)\n",
    "print(\"labels.json (len={}):\".format(len(labels)), labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c789a6c-071f-40ec-bd64-630fb8c39dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ true_divide (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TrueDivide</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ subtract (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Subtract</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ mobilenetv2_1.00_224 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,124</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ sequential (\u001b[38;5;33mSequential\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ true_divide (\u001b[38;5;33mTrueDivide\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ subtract (\u001b[38;5;33mSubtract\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ mobilenetv2_1.00_224 (\u001b[38;5;33mFunctional\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          │       \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                   │           \u001b[38;5;34m5,124\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,636,430</span> (21.50 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,636,430\u001b[0m (21.50 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,686,660</span> (6.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,686,660\u001b[0m (6.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">576,448</span> (2.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m576,448\u001b[0m (2.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,373,322</span> (12.87 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m3,373,322\u001b[0m (12.87 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape: (None, 4)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "MODEL_PATH = os.path.join(\"outputs\",\"checkpoints\",\"modelB_phase2_best.keras\")\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "model.summary()  # look at the LAST Dense: units must be 4\n",
    "print(\"Model output shape:\", model.output_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4375f25-1355-4e1c-91c3-7a104509c997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.17.0\n",
      "Working dir: C:\\cv_partb\n",
      "MODEL_PATH exists? True\n",
      "LABELS_PATH exists? True\n"
     ]
    }
   ],
   "source": [
    "# Paths (edit if yours are different)\n",
    "MODEL_PATH = r\"outputs/checkpoints/modelB_phase2_best.keras\"\n",
    "LABELS_PATH = r\"labels.json\"\n",
    "\n",
    "import os, sys, json, glob, pathlib, textwrap, hashlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Working dir:\", os.getcwd())\n",
    "print(\"MODEL_PATH exists?\", os.path.exists(MODEL_PATH))\n",
    "print(\"LABELS_PATH exists?\", os.path.exists(LABELS_PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c07b45-186c-4a1b-b96e-07fe5266d73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.json len = 4\n",
      "labels.json = ['Tomato_Bacterial_spot', 'Tomato_Early_blight', 'Tomato_Late_blight', 'Tomato_healthy']\n",
      "0: Tomato_Bacterial_spot\n",
      "1: Tomato_Early_blight\n",
      "2: Tomato_Late_blight\n",
      "3: Tomato_healthy\n"
     ]
    }
   ],
   "source": [
    "with open(LABELS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "print(\"labels.json len =\", len(labels))\n",
    "print(\"labels.json =\", labels)\n",
    "\n",
    "# Helpful: show one-class-per-line so you can visually inspect the order.\n",
    "for i, name in enumerate(labels):\n",
    "    print(f\"{i}: {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f34dc66a-de8e-407f-8bb0-dd6ec8636713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ true_divide (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TrueDivide</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ subtract (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Subtract</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ mobilenetv2_1.00_224 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,124</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ sequential (\u001b[38;5;33mSequential\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ true_divide (\u001b[38;5;33mTrueDivide\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ subtract (\u001b[38;5;33mSubtract\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ mobilenetv2_1.00_224 (\u001b[38;5;33mFunctional\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          │       \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                   │           \u001b[38;5;34m5,124\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,636,430</span> (21.50 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,636,430\u001b[0m (21.50 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,686,660</span> (6.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,686,660\u001b[0m (6.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">576,448</span> (2.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m576,448\u001b[0m (2.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,373,322</span> (12.87 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m3,373,322\u001b[0m (12.87 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape: (None, 4)\n",
      "OK: Model output units == len(labels)\n"
     ]
    }
   ],
   "source": [
    "m = tf.keras.models.load_model(MODEL_PATH)\n",
    "m.summary()\n",
    "print(\"Model output shape:\", m.output_shape)\n",
    "assert m.output_shape[-1] == len(labels), \\\n",
    "    f\"Mismatch! Model outputs {m.output_shape[-1]} but labels.json has {len(labels)}\"\n",
    "print(\"OK: Model output units == len(labels)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5601eb81-2765-41b2-9c38-57c63aed1899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders under data\\train => ['Tomato_Bacterial_spot', 'Tomato_Early_blight', 'Tomato_Late_blight', 'Tomato_healthy'] len= 4\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR_TRAIN = Path(\"data/train\")  # <-- EDIT if different\n",
    "\n",
    "if DATA_DIR_TRAIN.exists():\n",
    "    classes_found = sorted([p.name for p in DATA_DIR_TRAIN.iterdir() if p.is_dir()])\n",
    "    print(\"Folders under\", DATA_DIR_TRAIN, \"=>\", classes_found, \"len=\", len(classes_found))\n",
    "else:\n",
    "    print(\"WARN: DATA_DIR_TRAIN not found. Edit DATA_DIR_TRAIN to the correct path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d444392c-8fa3-456d-83c2-4f127b02636b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scanning: data\\test\n",
      "\n",
      "Class: Tomato_Bacterial_spot\n",
      "File: data\\test\\Tomato_Bacterial_spot\\887eab5c-9475-46b7-81df-ea2c5190e7fe___GCREC_Bact.Sp 6026.JPG\n",
      "Pred → Tomato_Early_blight  (conf=0.974)\n",
      "{'Tomato_Bacterial_spot': 1.4047498098079814e-07, 'Tomato_Early_blight': 0.974284827709198, 'Tomato_Late_blight': 0.025506658479571342, 'Tomato_healthy': 0.00020833595772273839}\n",
      "\n",
      "Class: Tomato_Early_blight\n",
      "File: data\\test\\Tomato_Early_blight\\484250ea-cc2c-4614-bee8-7f24007e00ac___RS_Erly.B 9463.JPG\n",
      "Pred → Tomato_Early_blight  (conf=0.978)\n",
      "{'Tomato_Bacterial_spot': 1.3557333033986652e-07, 'Tomato_Early_blight': 0.9779013395309448, 'Tomato_Late_blight': 0.021934427320957184, 'Tomato_healthy': 0.00016405197675339878}\n",
      "\n",
      "Class: Tomato_Late_blight\n",
      "File: data\\test\\Tomato_Late_blight\\df7775b2-73f3-4708-86d5-a56e2bbb3337___RS_Late.B 5378.JPG\n",
      "Pred → Tomato_Early_blight  (conf=0.974)\n",
      "{'Tomato_Bacterial_spot': 2.399609968506411e-07, 'Tomato_Early_blight': 0.9741044640541077, 'Tomato_Late_blight': 0.025662492960691452, 'Tomato_healthy': 0.0002328190894331783}\n",
      "\n",
      "Class: Tomato_healthy\n",
      "File: data\\test\\Tomato_healthy\\cb4a744a-1f1d-4cb1-b2a4-f1e1a12426ca___RS_HL 9970.JPG\n",
      "Pred → Tomato_Early_blight  (conf=0.975)\n",
      "{'Tomato_Bacterial_spot': 5.544311534322333e-07, 'Tomato_Early_blight': 0.9747410416603088, 'Tomato_Late_blight': 0.025016045197844505, 'Tomato_healthy': 0.00024224065418820828}\n",
      "\n",
      "Scanning: data_raw\n",
      "\n",
      "Class: Tomato_Bacterial_spot\n",
      "File: data_raw\\Tomato_Bacterial_spot\\ccf403ce-5219-4735-a7c3-709d3fc6590b___GCREC_Bact.Sp 3301.JPG\n",
      "Pred → Tomato_Early_blight  (conf=0.974)\n",
      "{'Tomato_Bacterial_spot': 1.7058168566563836e-07, 'Tomato_Early_blight': 0.9740316271781921, 'Tomato_Late_blight': 0.025741174817085266, 'Tomato_healthy': 0.0002270211698487401}\n",
      "\n",
      "Class: Tomato_Early_blight\n",
      "File: data_raw\\Tomato_Early_blight\\28d9f1bd-ee80-46e1-b89c-5a855f0c8400___RS_Erly.B 7818.JPG\n",
      "Pred → Tomato_Early_blight  (conf=0.971)\n",
      "{'Tomato_Bacterial_spot': 3.9809310692362487e-07, 'Tomato_Early_blight': 0.9710280895233154, 'Tomato_Late_blight': 0.028724174946546555, 'Tomato_healthy': 0.0002472609921824187}\n",
      "\n",
      "Class: Tomato_Late_blight\n",
      "File: data_raw\\Tomato_Late_blight\\d08bee32-4f76-4a08-9158-69fe55da2ddb___GHLB Leaf 23 Day 12.JPG\n",
      "Pred → Tomato_Early_blight  (conf=0.969)\n",
      "{'Tomato_Bacterial_spot': 1.6548101200442034e-07, 'Tomato_Early_blight': 0.9694616198539734, 'Tomato_Late_blight': 0.03028373047709465, 'Tomato_healthy': 0.0002544468152336776}\n",
      "\n",
      "Class: Tomato_healthy\n",
      "File: data_raw\\Tomato_healthy\\dd7a8ade-2bc7-4b28-9e27-f823e8466a76___GH_HL Leaf 236.JPG\n",
      "Pred → Tomato_Early_blight  (conf=0.969)\n",
      "{'Tomato_Bacterial_spot': 3.20502834938452e-07, 'Tomato_Early_blight': 0.9685071110725403, 'Tomato_Late_blight': 0.0311873909085989, 'Tomato_healthy': 0.0003052049723919481}\n",
      "\n",
      "Scanning: data\\val\n",
      "\n",
      "Class: Tomato_Bacterial_spot\n",
      "File: data\\val\\Tomato_Bacterial_spot\\4dcc0165-a3c5-4290-8262-16c63b677a7d___GCREC_Bact.Sp 3602.JPG\n",
      "Pred → Tomato_Early_blight  (conf=0.976)\n",
      "{'Tomato_Bacterial_spot': 1.947366996546407e-07, 'Tomato_Early_blight': 0.9764967560768127, 'Tomato_Late_blight': 0.02327951416373253, 'Tomato_healthy': 0.00022355598048307002}\n",
      "\n",
      "Class: Tomato_Early_blight\n",
      "File: data\\val\\Tomato_Early_blight\\ef68398c-ec92-4e44-961a-5ec1a29d3c28___RS_Erly.B 9620.JPG\n",
      "Pred → Tomato_Early_blight  (conf=0.979)\n",
      "{'Tomato_Bacterial_spot': 1.6500327149060467e-07, 'Tomato_Early_blight': 0.9786575436592102, 'Tomato_Late_blight': 0.021167639642953873, 'Tomato_healthy': 0.00017462187679484487}\n",
      "\n",
      "Class: Tomato_Late_blight\n",
      "File: data\\val\\Tomato_Late_blight\\09cdb8a4-0c20-4acd-b4c8-709fad7010fd___GHLB_PS Leaf 37 Day 16.jpg\n",
      "Pred → Tomato_Early_blight  (conf=0.968)\n",
      "{'Tomato_Bacterial_spot': 2.4936346676440735e-07, 'Tomato_Early_blight': 0.9683628678321838, 'Tomato_Late_blight': 0.031392667442560196, 'Tomato_healthy': 0.0002442400436848402}\n",
      "\n",
      "Class: Tomato_healthy\n",
      "File: data\\val\\Tomato_healthy\\62e5933a-1405-4f56-b5c6-3bb6996465f5___RS_HL 0378.JPG\n",
      "Pred → Tomato_Early_blight  (conf=0.968)\n",
      "{'Tomato_Bacterial_spot': 5.740364485973259e-07, 'Tomato_Early_blight': 0.9677379727363586, 'Tomato_Late_blight': 0.032095830887556076, 'Tomato_healthy': 0.00016562145901843905}\n",
      "\n",
      "Scanning: data\n"
     ]
    }
   ],
   "source": [
    "from random import choice\n",
    "\n",
    "def preprocess_pil(img_pil, size=(224,224)):\n",
    "    img = img_pil.convert(\"RGB\").resize(size)\n",
    "    x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    x = preprocess_input(x)  # MobileNetV2 preprocess\n",
    "    return np.expand_dims(x, 0)\n",
    "\n",
    "def predict_path(model, labels, path):\n",
    "    img = Image.open(path)\n",
    "    x = preprocess_pil(img, size=(224,224))\n",
    "    probs = model.predict(x, verbose=0)[0]\n",
    "    idx = int(np.argmax(probs))\n",
    "    top = labels[idx]\n",
    "    conf = float(probs[idx])\n",
    "    dist = {labels[i]: float(probs[i]) for i in range(len(labels))}\n",
    "    return top, conf, dist\n",
    "\n",
    "# Try to locate test images under data/test/<class>/... or data_raw/<class>/...\n",
    "CANDIDATE_TEST_DIRS = [Path(\"data/test\"), Path(\"data_raw\"), Path(\"data/val\"), Path(\"data\")]\n",
    "found_any = False\n",
    "\n",
    "for base in CANDIDATE_TEST_DIRS:\n",
    "    if not base.exists():\n",
    "        continue\n",
    "    print(\"\\nScanning:\", base)\n",
    "    for cls in labels:\n",
    "        class_dir = base / cls\n",
    "        if class_dir.exists():\n",
    "            files = [str(p) for p in class_dir.glob(\"*\") if p.suffix.lower() in {\".jpg\",\".jpeg\",\".png\",\".bmp\"}]\n",
    "            if files:\n",
    "                found_any = True\n",
    "                path = choice(files)\n",
    "                top, conf, dist = predict_path(m, labels, path)\n",
    "                print(f\"\\nClass: {cls}\\nFile: {path}\\nPred → {top}  (conf={conf:.3f})\")\n",
    "                print(dist)\n",
    "\n",
    "if not found_any:\n",
    "    print(\"No test images found under known locations. Point CANDIDATE_TEST_DIRS to your test folders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a52ebe2-fbd5-4348-9fe7-aaf2809d8f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FOUND] MODEL_PATH\n",
      "[FOUND] preprocess_input\n",
      "[FOUND] resize\n",
      "[FOUND] labels.json\n",
      "[FOUND] checkpoints\n",
      "[FOUND] predict(\n",
      "\n",
      "--- snippet (first 200 lines) ---\n",
      "\n",
      "import os, json\n",
      "import numpy as np\n",
      "import streamlit as st\n",
      "from PIL import Image\n",
      "import tensorflow as tf\n",
      "from tensorflow.keras.preprocessing import image as kimage\n",
      "\n",
      "MODEL_PATH  = os.path.join(\"outputs\", \"checkpoints\", \"modelB_phase2_best.keras\")\n",
      "LABELS_PATH = \"labels.json\"\n",
      "IMG_SIZE    = (224, 224)\n",
      "MIN_BYTES   = 1_000_000\n",
      "# ---------------------------------------\n",
      "\n",
      "def ensure_dirs():\n",
      "    os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
      "\n",
      "def ensure_model_local():\n",
      "    \"\"\"\n",
      "    Ensure the model file exists locally.\n",
      "    If missing or too small, prompt user to upload the .keras file (one-time).\n",
      "    \"\"\"\n",
      "    ensure_dirs()\n",
      "    needs_model = (not os.path.exists(MODEL_PATH)) or (os.path.getsize(MODEL_PATH) < MIN_BYTES)\n",
      "\n",
      "    if needs_model:\n",
      "        st.warning(\n",
      "            \"Model file is missing on this server. \"\n",
      "            \"Please upload your **.keras** weights (e.g., modelB_phase2_best.keras).\"\n",
      "        )\n",
      "        uploaded_model = st.file_uploader(\"Upload model file (.keras)\", type=[\"keras\", \"h5\", \"pb\"], key=\"model_uploader\")\n",
      "        if uploaded_model is not None:\n",
      "            try:\n",
      "                with open(MODEL_PATH, \"wb\") as f:\n",
      "                    f.write(uploaded_model.getbuffer())\n",
      "                st.success(f\"Model saved to {MODEL_PATH}.\")\n",
      "            except Exception as e:\n",
      "                st.error(f\"Failed to save uploaded model: {e}\")\n",
      "                st.stop()\n",
      "\n",
      "    if not os.path.exists(MODEL_PATH) or os.path.getsize(MODEL_PATH) < MIN_BYTES:\n",
      "        st.error(\n",
      "            f\"Model not available at {MODEL_PATH}. \"\n",
      "            \"Commit the model via Git LFS **or** upload it above.\"\n",
      "        )\n",
      "        st.stop()\n",
      "\n",
      "@st.cache_resource(show_spinner=False)\n",
      "def load_model_and_labels():\n",
      "    ensure_model_local()\n",
      "\n",
      "    if not os.path.exists(LABELS_PATH):\n",
      "        raise FileNotFoundError(f\"labels.json not found at: {LABELS_PATH}\")\n",
      "\n",
      "    try:\n",
      "        model = tf.keras.models.load_model(MODEL_PATH)\n",
      "    except Exception as e:\n",
      "        raise ValueError(f\"Could not load model from {MODEL_PATH}: {e}\")\n",
      "\n",
      "    with open(LABELS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
      "        labels = json.load(f)\n",
      "\n",
      "    try:\n",
      "        dummy = np.zeros((1, *IMG_SIZE, 3), dtype=np.float32)\n",
      "        dummy = tf.keras.applications.mobilenet_v2.preprocess_input(dummy)\n",
      "        out = model.predict(dummy, verbose=0)\n",
      "        if out.shape[-1] != len(labels):\n",
      "            raise ValueError(\n",
      "                f\"Model output classes ({out.shape[-1]}) != labels length ({len(labels)}). \"\n",
      "                \"Ensure labels.json order matches training class_names.\"\n",
      "            )\n",
      "    except Exception:\n",
      "        pass\n",
      "\n",
      "    return model, labels\n",
      "\n",
      "def preprocess_pil(img_pil):\n",
      "    img = img_pil.convert(\"RGB\").resize(IMG_SIZE)\n",
      "    arr = kimage.img_to_array(img)\n",
      "    arr = tf.keras.applications.mobilenet_v2.preprocess_input(arr)\n",
      "    arr = np.expand_dims(arr, axis=0)\n",
      "    return arr\n",
      "\n",
      "def predict(img_pil, model, labels):\n",
      "    arr = preprocess_pil(img_pil)\n",
      "    probs = model.predict(arr, verbose=0)[0]\n",
      "    idx = int(np.argmax(probs))\n",
      "    top_label = labels[idx]\n",
      "    top_conf = float(probs[idx]) * 100.0\n",
      "    order = np.argsort(probs)[::-1]\n",
      "    ranked = [(labels[i], float(probs[i]) * 100.0) for i in order]\n",
      "    return top_label, top_conf, ranked\n",
      "\n",
      "st.set_page_config(page_title=\"Plant Disease Detector (Part B)\", page_icon=\"🌿\")\n",
      "st.title(\"🌿 Plant Disease Detector — Part B\")\n",
      "st.caption(\"MobileNetV2 (transfer learning). Upload a leaf image to get a prediction.\")\n",
      "\n",
      "try:\n",
      "    with st.spinner(\"Loading model…\"):\n",
      "        model, labels = load_model_and_labels()\n",
      "except Exception as e:\n",
      "    st.error(str(e))\n",
      "    st.stop()\n",
      "\n",
      "uploaded = st.file_uploader(\"Upload a leaf image (JPG/PNG)\", type=[\"jpg\", \"jpeg\", \"png\"], key=\"image_uploader\")\n",
      "\n",
      "if uploaded is not None:\n",
      "    try:\n",
      "        img = Image.open(uploaded)\n",
      "        st.image(img, caption=\"Uploaded image\", use_column_width=True)\n",
      "\n",
      "        with st.spinner(\"Analyzing…\"):\n",
      "            top_label, top_conf, ranked = predict(img, model, labels)\n",
      "\n",
      "        st.subheader(\"Result\")\n",
      "        st.write(f\"**Prediction:** {top_label}\")\n",
      "        st.write(f\"**Confidence:** {top_conf:.2f}%\")\n",
      "\n",
      "        st.markdown(\"### All class probabilities\")\n",
      "        for name, conf in ranked:\n",
      "            st.write(f\"- {name}: {conf:.2f}%\")\n",
      "\n",
      "        if top_conf < 50:\n",
      "            st.info(\"Low confidence — try a clearer image or better lighting.\")\n",
      "    except Exception as e:\n",
      "        st.error(f\"Failed to process the image: {e}\")\n",
      "\n",
      "st.markdown(\"---\")\n",
      "st.caption(\"Part B — Computer Vision Coursework • Streamlit + TensorFlow (MobileNetV2)\")\n"
     ]
    }
   ],
   "source": [
    "APP_PATH = Path(\"streamlit_app.py\")\n",
    "if APP_PATH.exists():\n",
    "    app_code = APP_PATH.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    # Show the model path & preprocess calls found\n",
    "    for key in [\"MODEL_PATH\", \"preprocess_input\", \"resize\", \"labels.json\", \"checkpoints\", \"predict(\"]:\n",
    "        if key in app_code:\n",
    "            print(f\"[FOUND] {key}\")\n",
    "    print(\"\\n--- snippet (first 200 lines) ---\\n\")\n",
    "    print(\"\\n\".join(app_code.splitlines()[:200]))\n",
    "else:\n",
    "    print(\"streamlit_app.py not found here\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc740116-097a-4315-a0d0-59fdcfb2538c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KERAS FILES:\n",
      "  outputs\\checkpoints\\modelA_best.keras md5: 704e69bdb657db0f81f55083ecd32330 size: 1171061\n",
      "  outputs\\checkpoints\\modelA_best_v2.keras md5: 2d2e5ae8be1392978f1765cbc1e0cfd1 size: 1171289\n",
      "  outputs\\checkpoints\\modelB_phase1_best.keras md5: 5285be26b355bad8b30280a1fb58afeb size: 9695221\n",
      "  outputs\\checkpoints\\modelB_phase2_best.keras md5: db2f1876540b0c071387d2cb733c5402 size: 23177500\n",
      "\n",
      "LABELS.JSON FILES:\n",
      "  labels.json md5: a5264e7cb2be2cd92c28adec4eda7d26 size: 90\n"
     ]
    }
   ],
   "source": [
    "def md5(path, blocksize=1<<20):\n",
    "    m = hashlib.md5()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(blocksize)\n",
    "            if not b: break\n",
    "            m.update(b)\n",
    "    return m.hexdigest()\n",
    "\n",
    "# Find all *.keras and all labels.json in repo (recursively)\n",
    "keras_files = [str(p) for p in Path(\".\").rglob(\"*.keras\")]\n",
    "label_files = [str(p) for p in Path(\".\").rglob(\"labels.json\")]\n",
    "\n",
    "print(\"KERAS FILES:\")\n",
    "for p in keras_files:\n",
    "    try:\n",
    "        print(\" \", p, \"md5:\", md5(p), \"size:\", os.path.getsize(p))\n",
    "    except Exception as e:\n",
    "        print(\" \", p, \" (error reading md5)\", e)\n",
    "\n",
    "print(\"\\nLABELS.JSON FILES:\")\n",
    "for p in label_files:\n",
    "    try:\n",
    "        print(\" \", p, \"md5:\", md5(p), \"size:\", os.path.getsize(p))\n",
    "    except Exception as e:\n",
    "        print(\" \", p, \" (error reading md5)\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73f1f414-9247-43c8-9bdd-b887829d258a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using test base: data\\test\n",
      "\n",
      "Mini confusion (counts):\n",
      "true\\pred                 Tomato_Bacterial_spo  Tomato_Early_blight   Tomato_Late_blight    Tomato_healthy      \n",
      "Tomato_Bacterial_spot       0                     10                    0                     0                   \n",
      "Tomato_Early_blight         0                     10                    0                     0                   \n",
      "Tomato_Late_blight          0                     10                    0                     0                   \n",
      "Tomato_healthy              0                     10                    0                     0                   \n",
      "\n",
      "Total samples: 40\n"
     ]
    }
   ],
   "source": [
    "import random, collections\n",
    "\n",
    "def sample_paths(base, cls, k=10):\n",
    "    p = Path(base)/cls\n",
    "    if not p.exists(): return []\n",
    "    files = [str(x) for x in p.glob(\"*\") if x.suffix.lower() in {\".jpg\",\".jpeg\",\".png\",\".bmp\"}]\n",
    "    random.shuffle(files)\n",
    "    return files[:k]\n",
    "\n",
    "# Choose a base test dir that actually has images:\n",
    "BASE = None\n",
    "for c in CANDIDATE_TEST_DIRS:\n",
    "    if (c/labels[0]).exists():\n",
    "        BASE = c\n",
    "        break\n",
    "\n",
    "if BASE is None:\n",
    "    print(\"No suitable test dir found; skip this cell or set BASE manually.\")\n",
    "else:\n",
    "    print(\"Using test base:\", BASE)\n",
    "    counts = collections.Counter()\n",
    "    total = 0\n",
    "    for cls in labels:\n",
    "        for path in sample_paths(BASE, cls, k=10):\n",
    "            pred, conf, dist = predict_path(m, labels, path)\n",
    "            counts[(cls, pred)] += 1\n",
    "            total += 1\n",
    "\n",
    "    # Pretty print\n",
    "    print(\"\\nMini confusion (counts):\")\n",
    "    header = \"true\\\\pred\".ljust(24) + \"  \" + \"  \".join([p[:20].ljust(20) for p in labels])\n",
    "    print(header)\n",
    "    for t in labels:\n",
    "        row = [str(counts.get((t,p),0)).ljust(20) for p in labels]\n",
    "        print(t[:24].ljust(24), \"  \", \"  \".join(row))\n",
    "    print(\"\\nTotal samples:\", total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44bc93e8-5ee7-44a2-9ebf-88440668e92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Raw output: [2.2489110e-07 9.6904469e-01 3.0674538e-02 2.8054995e-04]\n",
      "Argmax: 1\n"
     ]
    }
   ],
   "source": [
    "# Load a test image from known class\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "img_path = \"data/test/Tomato_Late_blight/2d583f2c-3e59-4c4c-be45-5f367a38af95___GHLB2 Leaf 8724.JPG\"  # <-- CHANGE to any image\n",
    "img = Image.open(img_path).convert(\"RGB\").resize((224,224))\n",
    "x = image.img_to_array(img)\n",
    "x = preprocess_input(x)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "# Load model\n",
    "model = tf.keras.models.load_model(\"outputs/checkpoints/modelB_phase2_best.keras\")\n",
    "pred = model.predict(x)[0]\n",
    "print(\"Raw output:\", pred)\n",
    "print(\"Argmax:\", np.argmax(pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "658526bc-7083-4234-8d01-a0203efb09e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.json = ['Tomato_Bacterial_spot', 'Tomato_Early_blight', 'Tomato_Late_blight', 'Tomato_healthy']\n",
      "0: Tomato_Bacterial_spot\n",
      "1: Tomato_Early_blight\n",
      "2: Tomato_Late_blight\n",
      "3: Tomato_healthy\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"labels.json\", \"r\") as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "print(\"labels.json =\", labels)\n",
    "for i, lbl in enumerate(labels):\n",
    "    print(f\"{i}: {lbl}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9e66f3c-a4e6-4001-bb01-d47f9b3442a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: Tomato_Early_blight\n"
     ]
    }
   ],
   "source": [
    "top_idx = np.argmax(pred)\n",
    "print(\"Predicted label:\", labels[top_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cf20baf-3a28-4d05-8737-63c98d15ea79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tomato_Bacterial_spot', 'Tomato_Early_blight', 'Tomato_Late_blight', 'Tomato_healthy']\n",
      "✅ Saved labels.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "base = \"data/train\"  # or wherever your training data folders are\n",
    "labels = sorted(os.listdir(base))  # alphabetical order used by Keras generators\n",
    "print(labels)\n",
    "with open(\"labels.json\", \"w\") as f:\n",
    "    json.dump(labels, f)\n",
    "print(\"✅ Saved labels.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98929244-804d-4753-b5bc-69120907b899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.json: ['Tomato_Bacterial_spot', 'Tomato_Early_blight', 'Tomato_Late_blight', 'Tomato_healthy'] len = 4\n",
      "0 Tomato_Bacterial_spot\n",
      "1 Tomato_Early_blight\n",
      "2 Tomato_Late_blight\n",
      "3 Tomato_healthy\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"labels.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    labels = json.load(f)\n",
    "print(\"labels.json:\", labels, \"len =\", len(labels))\n",
    "for i,n in enumerate(labels): print(i, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0431aff4-d2a7-489d-b36d-c31ff231eca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prediction Result\n",
      "Image: data/test/Tomato_Late_blight/1a473cad-42fc-48ca-963c-d438fbca928f___RS_Late.B 5418.JPG\n",
      "Predicted Class: Tomato_Late_blight\n",
      "Confidence: 97.53%\n",
      "\n",
      "All Class Probabilities:\n",
      "- Tomato_Bacterial_spot: 0.00%\n",
      "- Tomato_Early_blight: 2.47%\n",
      "- Tomato_Late_blight: 97.53%\n",
      "- Tomato_healthy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "import json\n",
    "\n",
    "# Load model and labels\n",
    "model = tf.keras.models.load_model(\"outputs/checkpoints/modelB_phase2_best.keras\")\n",
    "with open(\"labels.json\", \"r\") as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "# === STEP 1: CHANGE THE IMAGE PATH BELOW ===\n",
    "img_path = \"data/test/Tomato_Late_blight/1a473cad-42fc-48ca-963c-d438fbca928f___RS_Late.B 5418.JPG\"\n",
    "\n",
    "# === STEP 2: Prediction ===\n",
    "img = Image.open(img_path).convert(\"RGB\").resize((224, 224))\n",
    "x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "x = preprocess_input(x)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "# Predict\n",
    "pred = model.predict(x, verbose=0)[0]\n",
    "predicted_index = int(np.argmax(pred))\n",
    "predicted_label = labels[predicted_index]\n",
    "confidence = float(pred[predicted_index]) * 100.0\n",
    "\n",
    "# Output\n",
    "print(\"✅ Prediction Result\")\n",
    "print(\"Image:\", img_path)\n",
    "print(\"Predicted Class:\", predicted_label)\n",
    "print(\"Confidence: {:.2f}%\".format(confidence))\n",
    "print(\"\\nAll Class Probabilities:\")\n",
    "for i, prob in enumerate(pred):\n",
    "    print(f\"- {labels[i]}: {prob*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c149318a-73a5-4752-85eb-5db44599843f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape: (None, 4)\n",
      "✅ Units match labels: 4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf, os, json\n",
    "MODEL_PATH = os.path.join(\"outputs\",\"checkpoints\",\"modelB_phase2_best.keras\")\n",
    "\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "print(\"Model output shape:\", model.output_shape)   # expect (..., 4)\n",
    "\n",
    "with open(\"labels.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    labels = json.load(f)\n",
    "assert model.output_shape[-1] == len(labels), \"Mismatch: model units vs label count!\"\n",
    "print(\"✅ Units match labels:\", len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1a2f5ba-b257-48a5-978e-87362ab0eb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 16 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000023F6EA30040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "True: Tomato_Bacterial_spot\n",
      "File: data\\test\\Tomato_Bacterial_spot\\bf962eca-a579-4f9d-aab4-7406179290b9___GCREC_Bact.Sp 6283.JPG\n",
      "Pred: Tomato_Early_blight  (conf=0.974)\n",
      "{'Tomato_Bacterial_spot': 1.6923151235914702e-07, 'Tomato_Early_blight': 0.9736642241477966, 'Tomato_Late_blight': 0.026117566972970963, 'Tomato_healthy': 0.00021805161668453366}\n",
      "\n",
      "True: Tomato_Early_blight\n",
      "File: data\\test\\Tomato_Early_blight\\484250ea-cc2c-4614-bee8-7f24007e00ac___RS_Erly.B 9463.JPG\n",
      "Pred: Tomato_Early_blight  (conf=0.978)\n",
      "{'Tomato_Bacterial_spot': 1.3557333033986652e-07, 'Tomato_Early_blight': 0.9779013395309448, 'Tomato_Late_blight': 0.021934427320957184, 'Tomato_healthy': 0.00016405197675339878}\n",
      "\n",
      "True: Tomato_Late_blight\n",
      "File: data\\test\\Tomato_Late_blight\\4b115bc3-911c-4e98-b948-a6a360f06a87___GHLB_PS Leaf 25 Day 9.jpg\n",
      "Pred: Tomato_Early_blight  (conf=0.968)\n",
      "{'Tomato_Bacterial_spot': 2.5620761334721465e-07, 'Tomato_Early_blight': 0.9675661325454712, 'Tomato_Late_blight': 0.0322280116379261, 'Tomato_healthy': 0.000205545817152597}\n",
      "\n",
      "True: Tomato_healthy\n",
      "File: data\\test\\Tomato_healthy\\ee0dd8c0-8db2-4eff-8e4d-76cb146f2b34___GH_HL Leaf 166.2.JPG\n",
      "Pred: Tomato_Early_blight  (conf=0.973)\n",
      "{'Tomato_Bacterial_spot': 2.7743078589992365e-07, 'Tomato_Early_blight': 0.9725164771080017, 'Tomato_Late_blight': 0.027153778821229935, 'Tomato_healthy': 0.00032950847526080906}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, glob, random, os\n",
    "from PIL import Image\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "IMG_SIZE = (224,224)\n",
    "\n",
    "def preprocess_pil(img):\n",
    "    img = img.convert(\"RGB\").resize(IMG_SIZE)\n",
    "    x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    x = preprocess_input(x)\n",
    "    return np.expand_dims(x, 0)\n",
    "\n",
    "def predict_path(path):\n",
    "    img = Image.open(path)\n",
    "    x = preprocess_pil(img)\n",
    "    probs = model.predict(x, verbose=0)[0]\n",
    "    idx = int(np.argmax(probs))\n",
    "    return labels[idx], float(probs[idx]), dict(zip(labels, [float(p) for p in probs]))\n",
    "\n",
    "# test images from data/test/<class>/... or data_raw/<class>/...\n",
    "for cls in labels:\n",
    "    candidates = glob.glob(os.path.join(\"data\",\"test\",cls,\"*\")) or glob.glob(os.path.join(\"data_raw\",cls,\"*\"))\n",
    "    if not candidates:\n",
    "        print(f\"[WARN] no test images for {cls}\")\n",
    "        continue\n",
    "    p = random.choice(candidates)\n",
    "    pred, conf, dist = predict_path(p)\n",
    "    print(f\"\\nTrue: {cls}\\nFile: {p}\\nPred: {pred}  (conf={conf:.3f})\")\n",
    "    print(dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc836a5d-7153-4eae-ac40-1a938de7f7f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Inspect one batch from the training dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bx, by \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_ds\u001b[49m\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, bx\u001b[38;5;241m.\u001b[39mshape, bx\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, by\u001b[38;5;241m.\u001b[39mshape, by\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_ds' is not defined"
     ]
    }
   ],
   "source": [
    "# Inspect one batch from the training dataset\n",
    "for bx, by in train_ds.take(1):\n",
    "    print(\"X shape:\", bx.shape, bx.dtype)\n",
    "    print(\"Y shape:\", by.shape, by.dtype)\n",
    "    # Peek at first 3 labels\n",
    "    print(\"First 3 labels:\\n\", by[:3].numpy())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37dc0794-13d6-4f62-8a39-bfe14b4c172f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved labels.json: ['Tomato_Bacterial_spot', 'Tomato_Early_blight', 'Tomato_Late_blight', 'Tomato_healthy']\n",
      "Found 4636 files belonging to 4 classes.\n",
      "Found 993 files belonging to 4 classes.\n",
      "Found 997 files belonging to 4 classes.\n",
      "✅ Datasets ready.\n",
      "Train classes (forced order): ['Tomato_Bacterial_spot', 'Tomato_Early_blight', 'Tomato_Late_blight', 'Tomato_healthy']\n",
      "🔧 Phase 1 training (feature extraction)…\n",
      "Epoch 1/8\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691ms/step - accuracy: 0.6291 - loss: 0.8890\n",
      "Epoch 1: val_accuracy improved from None to 0.89527, saving model to outputs\\checkpoints\\modelB_phase1_best.keras\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 875ms/step - accuracy: 0.7640 - loss: 0.6039 - val_accuracy: 0.8953 - val_loss: 0.3014\n",
      "Epoch 2/8\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700ms/step - accuracy: 0.8922 - loss: 0.3190\n",
      "Epoch 2: val_accuracy improved from 0.89527 to 0.92850, saving model to outputs\\checkpoints\\modelB_phase1_best.keras\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 853ms/step - accuracy: 0.9014 - loss: 0.2905 - val_accuracy: 0.9285 - val_loss: 0.2178\n",
      "Epoch 3/8\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 698ms/step - accuracy: 0.9181 - loss: 0.2411\n",
      "Epoch 3: val_accuracy improved from 0.92850 to 0.94260, saving model to outputs\\checkpoints\\modelB_phase1_best.keras\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 847ms/step - accuracy: 0.9230 - loss: 0.2315 - val_accuracy: 0.9426 - val_loss: 0.1885\n",
      "Epoch 4/8\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710ms/step - accuracy: 0.9292 - loss: 0.2117\n",
      "Epoch 4: val_accuracy improved from 0.94260 to 0.94663, saving model to outputs\\checkpoints\\modelB_phase1_best.keras\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 859ms/step - accuracy: 0.9333 - loss: 0.2019 - val_accuracy: 0.9466 - val_loss: 0.1665\n",
      "Epoch 5/8\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708ms/step - accuracy: 0.9329 - loss: 0.1923\n",
      "Epoch 5: val_accuracy did not improve from 0.94663\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 852ms/step - accuracy: 0.9379 - loss: 0.1841 - val_accuracy: 0.9466 - val_loss: 0.1606\n",
      "Epoch 6/8\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692ms/step - accuracy: 0.9405 - loss: 0.1724\n",
      "Epoch 6: val_accuracy improved from 0.94663 to 0.94965, saving model to outputs\\checkpoints\\modelB_phase1_best.keras\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 842ms/step - accuracy: 0.9409 - loss: 0.1673 - val_accuracy: 0.9496 - val_loss: 0.1589\n",
      "Epoch 7/8\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700ms/step - accuracy: 0.9449 - loss: 0.1636\n",
      "Epoch 7: val_accuracy improved from 0.94965 to 0.95368, saving model to outputs\\checkpoints\\modelB_phase1_best.keras\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 853ms/step - accuracy: 0.9448 - loss: 0.1592 - val_accuracy: 0.9537 - val_loss: 0.1541\n",
      "Epoch 8/8\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691ms/step - accuracy: 0.9498 - loss: 0.1481\n",
      "Epoch 8: val_accuracy did not improve from 0.95368\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 842ms/step - accuracy: 0.9515 - loss: 0.1464 - val_accuracy: 0.9527 - val_loss: 0.1408\n",
      "🔧 Phase 2 training (fine-tuning)…\n",
      "Epoch 1/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 994ms/step - accuracy: 0.8895 - loss: 0.3329\n",
      "Epoch 1: val_accuracy improved from None to 0.80765, saving model to outputs\\checkpoints\\modelB_phase2_best.keras\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 1s/step - accuracy: 0.9312 - loss: 0.2001 - val_accuracy: 0.8077 - val_loss: 0.6217\n",
      "Epoch 2/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9887 - loss: 0.0382\n",
      "Epoch 2: val_accuracy improved from 0.80765 to 0.89728, saving model to outputs\\checkpoints\\modelB_phase2_best.keras\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 1s/step - accuracy: 0.9905 - loss: 0.0301 - val_accuracy: 0.8973 - val_loss: 0.3255\n",
      "Epoch 3/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 985ms/step - accuracy: 0.9933 - loss: 0.0250\n",
      "Epoch 3: val_accuracy improved from 0.89728 to 0.95670, saving model to outputs\\checkpoints\\modelB_phase2_best.keras\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 1s/step - accuracy: 0.9950 - loss: 0.0163 - val_accuracy: 0.9567 - val_loss: 0.1306\n",
      "Epoch 4/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980ms/step - accuracy: 0.9966 - loss: 0.0120\n",
      "Epoch 4: val_accuracy improved from 0.95670 to 0.97382, saving model to outputs\\checkpoints\\modelB_phase2_best.keras\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 1s/step - accuracy: 0.9963 - loss: 0.0125 - val_accuracy: 0.9738 - val_loss: 0.0781\n",
      "Epoch 5/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9965 - loss: 0.0094\n",
      "Epoch 5: val_accuracy did not improve from 0.97382\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 1s/step - accuracy: 0.9965 - loss: 0.0086 - val_accuracy: 0.9688 - val_loss: 0.1077\n",
      "Epoch 6/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9993 - loss: 0.0039\n",
      "Epoch 6: val_accuracy did not improve from 0.97382\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 1s/step - accuracy: 0.9991 - loss: 0.0041 - val_accuracy: 0.9627 - val_loss: 0.0972\n",
      "Epoch 7/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9986 - loss: 0.0055\n",
      "Epoch 7: val_accuracy did not improve from 0.97382\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 1s/step - accuracy: 0.9985 - loss: 0.0062 - val_accuracy: 0.9617 - val_loss: 0.1292\n",
      "Epoch 8/10\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 999ms/step - accuracy: 0.9982 - loss: 0.0048\n",
      "Epoch 8: val_accuracy did not improve from 0.97382\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 1s/step - accuracy: 0.9987 - loss: 0.0045 - val_accuracy: 0.9668 - val_loss: 0.1126\n",
      "✅ Saved best model to: outputs\\checkpoints\\modelB_phase2_best.keras\n",
      "✅ Model head units match labels: 4\n",
      "WARNING:tensorflow:6 out of the last 20 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000023F15DAADD0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "✅ Single-Image Prediction\n",
      "Image: data\\test\\Tomato_Late_blight\\1a473cad-42fc-48ca-963c-d438fbca928f___RS_Late.B 5418.JPG\n",
      "Predicted: Tomato_Late_blight | Confidence: 97.53%\n",
      "All probs: {'Tomato_Bacterial_spot': 9.61593826787066e-08, 'Tomato_Early_blight': 0.024675583466887474, 'Tomato_Late_blight': 0.975323498249054, 'Tomato_healthy': 8.376343316740531e-07}\n",
      "\n",
      "📊 Quick test set: acc=97.89%  loss=0.0739\n"
     ]
    }
   ],
   "source": [
    "# ==== ONE-CELL FIX: retrain correctly on 4 classes, save model+labels, sanity-check ====\n",
    "import os, json, pathlib, numpy as np, tensorflow as tf\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "\n",
    "# -----------------------------\n",
    "# 0) CONFIG – EDIT THESE PATHS\n",
    "# -----------------------------\n",
    "DATA_TRAIN = Path(\"data/train\")  # e.g. data/train/<class>/*\n",
    "DATA_VAL   = Path(\"data/val\")    # e.g. data/val/<class>/*\n",
    "DATA_TEST  = Path(\"data/test\")   # e.g. data/test/<class>/*\n",
    "TEST_IMG   = Path(\"data/test/Tomato_Late_blight/1a473cad-42fc-48ca-963c-d438fbca928f___RS_Late.B 5418.JPG\")  # change to any image you want to test\n",
    "\n",
    "# Fixed class list (keeps order consistent everywhere)\n",
    "CLASS_NAMES = [\n",
    "    \"Tomato_Bacterial_spot\",\n",
    "    \"Tomato_Early_blight\",\n",
    "    \"Tomato_Late_blight\",\n",
    "    \"Tomato_healthy\",\n",
    "]\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_PHASE1 = 8\n",
    "EPOCHS_PHASE2 = 10\n",
    "FINE_TUNE_UNFREEZE = 40  # unfreeze last N layers of MobileNetV2 in Phase 2\n",
    "CKPT_DIR = Path(\"outputs/checkpoints\"); CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_OUT = CKPT_DIR / \"modelB_phase2_best.keras\"\n",
    "LABELS_PATH = Path(\"labels.json\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1) SAVE labels.json (truth source)\n",
    "# ---------------------------------------\n",
    "with open(LABELS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(CLASS_NAMES, f, indent=2)\n",
    "print(\"✅ Saved labels.json:\", CLASS_NAMES)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 2) Build tf.data datasets with ONE-HOT labels\n",
    "#    (label_mode='categorical' => matches softmax + categorical_crossentropy)\n",
    "# ------------------------------------------------------\n",
    "def make_ds(root: Path, shuffle=True):\n",
    "    if not root.exists():\n",
    "        return None\n",
    "    ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        directory=str(root),\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"categorical\",              # ONE-HOT labels\n",
    "        class_names=CLASS_NAMES,               # FORCE this exact order\n",
    "        image_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=shuffle,\n",
    "        interpolation=\"bilinear\",\n",
    "    )\n",
    "    # Map MobileNetV2 preprocess ([-1,1]) to keep train==serve\n",
    "    ds = ds.map(lambda x, y: (preprocess_input(tf.cast(x, tf.float32)), y),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = make_ds(DATA_TRAIN, shuffle=True)\n",
    "val_ds   = make_ds(DATA_VAL,   shuffle=False)\n",
    "test_ds  = make_ds(DATA_TEST,  shuffle=False)\n",
    "\n",
    "if train_ds is None or val_ds is None:\n",
    "    raise SystemExit(\"❌ DATA_TRAIN or DATA_VAL not found. Please set DATA_TRAIN/DATA_VAL paths correctly.\")\n",
    "\n",
    "print(\"✅ Datasets ready.\")\n",
    "print(\"Train classes (forced order):\", CLASS_NAMES)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) Build MobileNetV2 model (ImageNet) + 4-class softmax head\n",
    "# -------------------------------------------------------------------\n",
    "base = MobileNetV2(input_shape=(224,224,3), include_top=False, weights=\"imagenet\")\n",
    "base.trainable = False  # Phase 1: feature extraction\n",
    "\n",
    "inputs = layers.Input(shape=(224,224,3))\n",
    "# NOTE: We already preprocessed in tf.data; so feed inputs directly.\n",
    "x = base(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.3)(x)  # you used 0.3\n",
    "outputs = layers.Dense(len(CLASS_NAMES), activation=\"softmax\")(x)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Compile with CORRECT loss for one-hot labels\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "ckpt_phase1 = CKPT_DIR / \"modelB_phase1_best.keras\"\n",
    "cb1 = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(str(ckpt_phase1), monitor=\"val_accuracy\",\n",
    "                                       mode=\"max\", save_best_only=True, verbose=1),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", mode=\"max\",\n",
    "                                     patience=3, restore_best_weights=True),\n",
    "]\n",
    "\n",
    "print(\"🔧 Phase 1 training (feature extraction)…\")\n",
    "hist1 = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS_PHASE1, callbacks=cb1, verbose=1)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 4) Fine-tune: unfreeze last N layers of the backbone\n",
    "# -----------------------------------------------------------\n",
    "base.trainable = True\n",
    "# Freeze all but last N layers\n",
    "for layer in base.layers[:-FINE_TUNE_UNFREEZE]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),  # smaller LR\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "cb2 = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(str(MODEL_OUT), monitor=\"val_accuracy\",\n",
    "                                       mode=\"max\", save_best_only=True, verbose=1),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", mode=\"max\",\n",
    "                                     patience=4, restore_best_weights=True),\n",
    "]\n",
    "\n",
    "print(\"🔧 Phase 2 training (fine-tuning)…\")\n",
    "hist2 = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS_PHASE2, callbacks=cb2, verbose=1)\n",
    "\n",
    "# --------------------------------------------\n",
    "# 5) Save final model (already saved best)\n",
    "# --------------------------------------------\n",
    "print(f\"✅ Saved best model to: {MODEL_OUT}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# 6) Sanity: check model units == labels\n",
    "# --------------------------------------------\n",
    "fixed = tf.keras.models.load_model(str(MODEL_OUT))\n",
    "assert fixed.output_shape[-1] == len(CLASS_NAMES), \\\n",
    "    f\"Output units {fixed.output_shape[-1]} != {len(CLASS_NAMES)} labels\"\n",
    "print(\"✅ Model head units match labels:\", fixed.output_shape[-1])\n",
    "\n",
    "# --------------------------------------------\n",
    "# 7) Single-image sanity prediction (edit TEST_IMG)\n",
    "# --------------------------------------------\n",
    "if TEST_IMG.exists():\n",
    "    img = Image.open(TEST_IMG).convert(\"RGB\").resize(IMG_SIZE)\n",
    "    arr = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    # NOTE: train pipeline already used preprocess_input, so we must do the same here:\n",
    "    arr = preprocess_input(arr)\n",
    "    arr = np.expand_dims(arr, axis=0)\n",
    "    probs = fixed.predict(arr, verbose=0)[0]\n",
    "    idx = int(np.argmax(probs))\n",
    "    print(\"\\n✅ Single-Image Prediction\")\n",
    "    print(\"Image:\", str(TEST_IMG))\n",
    "    print(\"Predicted:\", CLASS_NAMES[idx], \"| Confidence: {:.2f}%\".format(float(probs[idx])*100))\n",
    "    print(\"All probs:\", {CLASS_NAMES[i]: float(probs[i]) for i in range(len(CLASS_NAMES))})\n",
    "else:\n",
    "    print(f\"⚠️ TEST_IMG not found: {TEST_IMG}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# 8) Optional: quick test-set accuracy (if DATA_TEST exists)\n",
    "# --------------------------------------------\n",
    "if test_ds is not None:\n",
    "    loss, acc = fixed.evaluate(test_ds, verbose=0)\n",
    "    print(f\"\\n📊 Quick test set: acc={acc*100:.2f}%  loss={loss:.4f}\")\n",
    "else:\n",
    "    print(\"ℹ️ No test dataset found; skipped quick evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b9727-1d45-47d6-823e-e46a65821cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
